from snakemake_storage_plugin_http import HTTPTokenAuth
import os

envvars:
    "ACCESS_TOKEN",
    "RESULT_BUCKET",
    "INPUT_DIR",
    "OUTPUT_DIR",
    "TMP_DIR",
    "USER_ID",
    "INBOX_HOST",
    "DOWNLOAD_HOST",
    "WORKFLOW_ID" 

storage inbox:
    provider="s3",
    endpoint_url=os.environ["INBOX_HOST"],
    access_key=os.environ["USER_ID"],
    secret_key=os.environ["USER_ID"],
    token=os.environ["ACCESS_TOKEN"],

storage sai:
    provider="http",
    auth=HTTPTokenAuth(os.environ["ACCESS_TOKEN"])

INPUT_URL = os.path.join(os.environ["DOWNLOAD_HOST"], "s3", os.environ["INPUT_DIR"])
TMP_DIR = os.path.join(os.environ["TMP_DIR"], os.environ["WORKFLOW_ID"])
RESULT_URL = os.path.join("s3://", os.environ["RESULT_BUCKET"], os.environ["OUTPUT_DIR"])

configfile: "config/config.yaml"

containerized: "docker://axschmidt/gwas_pipeline:0.1"

rule all:
    input:
        storage.inbox(expand(RESULT_URL+"/results/regenie_association_merged/{phenotypes}.regenie.gz", phenotypes=config["phenotype"]["phenotype_columns"] )),
        storage.inbox(expand(RESULT_URL+"/results/regenie_association_merged_QC/{phenotypes}_LOG10P_manhattan.png", phenotypes=config["phenotype"]["phenotype_columns"] )) 

##### >>> file preparation #####

rule prune_common:
    input:
        fam=storage.sai(INPUT_URL+"/"+config["genotype"]["genotype_plink_prefix"] + ".fam"),
        bim=storage.sai(INPUT_URL+"/"+config["genotype"]["genotype_plink_prefix"] + ".bim"),
        bed=storage.sai(INPUT_URL+"/"+config["genotype"]["genotype_plink_prefix"] + ".bed"),
    output:
        pruned_variant_set=TMP_DIR+"/results/prune_common/prune_common.prune.in",
    # resources: cpus=1, mem_mb=18000, time_job=720, partition=config["medium_part"]
    params:		
        out_prefix=lambda wildcards, output: output["pruned_variant_set"][:-9],
    conda: "envs/plink.yaml"
    shell:
        """
        plink \
        --bed {input.bed} \
        --bim {input.bim} \
        --fam {input.fam} \
        --chr 1-22 \
        --indep-pairwise 1000 50 0.2 \
        --keep-allele-order \
        --maf 0.01 \
        --out "{params.out_prefix}"
        """


rule PCA_for_cov:
    input:
        fam=storage.sai(INPUT_URL+"/"+config["genotype"]["genotype_plink_prefix"] + ".fam"),
        bim=storage.sai(INPUT_URL+"/"+config["genotype"]["genotype_plink_prefix"] + ".bim"),
        bed=storage.sai(INPUT_URL+"/"+config["genotype"]["genotype_plink_prefix"] + ".bed"),
        pruned_variant_set=TMP_DIR+"/results/prune_common/prune_common.prune.in",
    output:
        eigenv=TMP_DIR+"/results/covar_PCA/common_vars.eigenvec"
    # resources: cpus=1, mem_mb=18000, time_job=720, partition=config["medium_part"],
    params:		
        out_prefix=lambda wildcards, output: output["eigenv"][:-9],
        covariates_nPC=config["phenotype"]["covariates_nPC"],




    conda: "envs/plink.yaml"

    shell: "plink --bed {input.bed} --bim {input.bim} --fam {input.fam} --extract {input.pruned_variant_set} --pca {params.covariates_nPC} --out '{params.out_prefix}'"


rule make_regenie_pheno:
    input:
        sample_sheet_file=storage.sai(INPUT_URL+"/"+config["phenotype"]["sample_sheet"]),
    output:
        pheno_file=TMP_DIR+"/results/pheno_cov/pheno.pheno",
    params: 
        pheno_columns=config["phenotype"]["phenotype_columns"],
    conda: "envs/R_tidyverse.yaml"
    # resources: cpus=1, mem_mb=5000, time_job=720, partition=config["medium_part"],
    script: "scripts/make_pheno.R"


rule make_regenie_cov:
    input:
        sample_sheet_file=storage.sai(INPUT_URL+"/"+config["phenotype"]["sample_sheet"]),
        cov_pcs_file=TMP_DIR+"/results/covar_PCA/common_vars.eigenvec",
    output:
        cov_file=TMP_DIR+"/results/pheno_cov/cov.cov",
    params: 
        cov_columns=config["phenotype"]["covariate_columns"],
        covariates_nPC=config["phenotype"]["covariates_nPC"],
    conda: "envs/R_tidyverse.yaml"
    # resources: cpus=1, mem_mb=5000, time_job=720, partition=config["medium_part"],
    script: "scripts/make_cov.R"


rule convert_imputed_vcf:
    input:
        imputed_vcf=storage.sai(expand(INPUT_URL+"/"+config["genotype"]["imp_prefix"] + "{contig}" + config["genotype"]["imp_postfix"], contig=config["genotype"]["imp_contigs"])),
    output:
        pgen=TMP_DIR+"/results/imp_converted/chr{contig}.pgen",
        psam=TMP_DIR+"/results/imp_converted/chr{contig}.psam",
        pvar=TMP_DIR+"/results/imp_converted/chr{contig}.pvar",
    params:
        plink_out=lambda wildcards, output: output["pgen"][:-5],
        dosage_param=config["genotype"]["dosage_param"]
    conda: "envs/plink2.yaml"
    shell:
        """
        plink2 \
        --vcf {input.imputed_vcf} dosage={params.dosage_param} \
        --double-id \
        --make-pgen \
        --threads 7 \
        --out {params.plink_out}
        """

##### <<< file preparation #####



##### >>> association analysis #####

rule regenie_association:
    input:
        cov=TMP_DIR+"/results/pheno_cov/cov.cov",
        pheno=TMP_DIR+"/results/pheno_cov/pheno.pheno",
        step2_pgen=expand(TMP_DIR+"/results/imp_converted/chr{contig}.pgen", contig=config["genotype"]["imp_contigs"]),
        step2_pvar=expand(TMP_DIR+"/results/imp_converted/chr{contig}.pvar", contig=config["genotype"]["imp_contigs"]),
        step2_psam=expand(TMP_DIR+"/results/imp_converted/chr{contig}.psam", contig=config["genotype"]["imp_contigs"])
    output:
        step2_NQ2=expand(TMP_DIR+"/results/regenie_association/{contig}_{phenotypes}.regenie.gz", 
            contig=config["genotype"]["imp_contigs"],
            phenotypes=config["phenotype"]["phenotype_columns"],
            allow_missing=True)
    params:
        plink_in=lambda wildcards, input: input["step2_pgen"][0][:-5],
        local_prefix=lambda widlcards, output: output["step2_NQ2"][0].split(TMP_DIR)[0],
        out_prefix=expand(TMP_DIR + "/results/regenie_association/{contig}", contig=config["genotype"]["imp_contigs"])
    conda: "envs/regenie.yaml"
    shell:
        """
        regenie \
        --step 2 \
        --pgen {params.plink_in} \
        --phenoFile {input.pheno} \
        --covarFile {input.cov} \
        --bt \
        --write-samples \
        --gz \
        --ignore-pred \
        --bsize 1000 \
        --minMAC 10 \
        --out {params.local_prefix}{params.out_prefix}
        """
        
rule merge_regenie_results:
    input:
        expand(TMP_DIR+"/results/regenie_association/{contig}_{phenotypes}.regenie.gz", contig=config["genotype"]["imp_contigs"], allow_missing=True), 
    output:
        merged_assoc=TMP_DIR+"/results/regenie_association_merged/{phenotypes}.regenie.gz",
    params:
        local_prefix=lambda widlcards, output: output["merged_assoc"].split(TMP_DIR)[0],
        header=TMP_DIR+"/results/regenie_association_merged/header_{phenotypes}.txt"
    conda: "envs/tabix.yaml"
    shell:
        """
        if zcat {input} | head -n1 > {params.local_prefix}{params.header}
        then
        echo "error"
        fi
        
        zcat {input} | grep -v "CHROM" | \
        cat {params.local_prefix}{params.header} - | \
        bgzip > {output}
        """

##### <<< association analysis #####





##### >>> QC of association results #####

rule generate_qq_plots:
    input:
        merged_assoc=TMP_DIR+"/results/regenie_association_merged/{phenotypes}.regenie.gz",
    output:
        out=TMP_DIR+"/results/regenie_association_merged_QC/{phenotypes}_LOG10P_manhattan.png"
    # resources: cpus=1, mem_mb=30000, time_job=720
    params:
        output_prefix=lambda wildcards, output: output["out"][:-21],
        bp_col="GENPOS",
        chr_col="CHROM",
        pcols="LOG10P",
    conda: "envs/R_qqplot.yaml"
    script: "scripts/qqplot.R"
##### <<< QC of association results #####


##### >>> upload results #####
rule upload_results:
    input:
        expand(TMP_DIR+"/results/regenie_association_merged/{phenotypes}.regenie.gz", phenotypes=config["phenotype"]["phenotype_columns"] ),
        expand(TMP_DIR+"/results/regenie_association_merged_QC/{phenotypes}_LOG10P_manhattan.png", phenotypes=config["phenotype"]["phenotype_columns"] )
    output:
        storage.inbox(expand(RESULT_URL+"/results/regenie_association_merged/{phenotypes}.regenie.gz", phenotypes=config["phenotype"]["phenotype_columns"] )),
        storage.inbox(expand(RESULT_URL+"/results/regenie_association_merged_QC/{phenotypes}_LOG10P_manhattan.png", phenotypes=config["phenotype"]["phenotype_columns"] )) 
    run:
        for src, dest in zip(input, output):
            shell("cp {src} {dest}")
        

